<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>How Many Websites Provide RSS / Web Syndication Feeds | concise notes</title>
<meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1">
<meta name="description" content="How to find out? Let&apos;s dig.">
<meta property="og:type" content="article">
<meta property="og:title" content="How Many Websites Provide RSS / Web Syndication Feeds">
<meta property="og:url" content="https://vhf.github.io/blog/2016/03/21/rss-usage-on-the-web/index.html">
<meta property="og:site_name" content="concise notes">
<meta property="og:description" content="How to find out? Let&apos;s dig.">
<meta property="og:updated_time" content="2016-03-21T22:38:23.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="How Many Websites Provide RSS / Web Syndication Feeds">
<meta name="twitter:description" content="How to find out? Let&apos;s dig.">
<meta name="twitter:creator" content="@_vhf">
<link rel="alternate" href="/blog/atom.xml" title="concise notes" type="application/atom+xml">
<link rel="shortcut icon" href="/favicons/favicon.ico">
<link rel="icon" sizes="16x16 32x32 64x64" href="/favicons/favicon.ico">
<link rel="icon" type="image/png" sizes="196x196" href="/favicons/favicon-192.png">
<link rel="icon" type="image/png" sizes="160x160" href="/favicons/favicon-160.png">
<link rel="icon" type="image/png" sizes="96x96" href="/favicons/favicon-96.png">
<link rel="icon" type="image/png" sizes="64x64" href="/favicons/favicon-64.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16.png">
<link rel="apple-touch-icon" href="/favicons/favicon-57.png">
<link rel="apple-touch-icon" sizes="114x114" href="/favicons/favicon-114.png">
<link rel="apple-touch-icon" sizes="72x72" href="/favicons/favicon-72.png">
<link rel="apple-touch-icon" sizes="144x144" href="/favicons/favicon-144.png">
<link rel="apple-touch-icon" sizes="60x60" href="/favicons/favicon-60.png">
<link rel="apple-touch-icon" sizes="120x120" href="/favicons/favicon-120.png">
<link rel="apple-touch-icon" sizes="76x76" href="/favicons/favicon-76.png">
<link rel="apple-touch-icon" sizes="152x152" href="/favicons/favicon-152.png">
<link rel="apple-touch-icon" sizes="180x180" href="/favicons/favicon-180.png">
<meta name="msapplication-TileColor" content="#FFFFFF">
<meta name="msapplication-TileImage" content="/favicons/favicon-144.png">
<meta name="msapplication-config" content="/favicons/browserconfig.xml">
<link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="/blog/css/style.css">
<script type="text/javascript">!function(e,a,n,t,c,i,o){e.GoogleAnalyticsObject=c,e[c]=e[c]||function(){(e[c].q=e[c].q||[]).push(arguments)},e[c].l=1*new Date,i=a.createElement(n),o=a.getElementsByTagName(n)[0],i.async=1,i.src=t,o.parentNode.insertBefore(i,o)}(window,document,"script","//www.google-analytics.com/analytics.js","ga"),ga("create","UA-69443473-1","auto"),ga("require","linkid"),ga("send","pageview")</script>
</head>
<body>
<div class="container">
<div class="page-wrap">
<header class="page-header" role="banner" itemscope="itemscope" itemtype="http://schema.org/WPHeader">
<nav itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
<div class="nav-wrapper">
<h1 class="brand-logo" itemprop="headline"><a href="/blog/" rel="home">concise notes</a></h1>
<meta itemprop="description" content="Victor Felder's blog">
<a href="#" id="mobile-menu" class="hide-on-med-and-up menu-icon"></a>
<nav role="navigation" itemscope="itemscope" itemtype="http://schema.org/SiteNavigationElement">
<ul class="right hide-on-small-only">
<li><a class="main-nav-link" href="/blog/">Home</a></li>
<li><a class="main-nav-link" href="/blog/archives">Archives</a></li>
<li><a class="main-nav-link" href="/blog/contact">Contact</a></li>
<li><a class="main-nav-link" href="https://vhf.github.io">About</a></li>
<li><a class="rss-icon" href="/blog/atom.xml" title="RSS Feed"></a></li>
</ul>
</nav>
</div>
</nav>
</header>
<div class="pattern"></div>
<main class="page-main" role="main" tabindex="-1">
<article id="post-rss-usage-on-the-web" class="article article-type-post" itemscope itemtype="http://schema.org/BlogPosting" itemprop="blogPost">
<link itemprop="mainEntityOfPage" href="/blog/2016/03/21/rss-usage-on-the-web/">
<nav class="hide-on-small-only">
<div class="nav-wrapper">
<div class="col s12">
<a href="/blog/2016/03/21/rss-usage-on-the-web/" class="breadcrumb">
<time datetime="2016-03-21T06:20:05.000Z" itemprop="datePublished">2016-03-21</time>
<time class="hide" datetime="2016-03-21T06:20:05.000Z" itemprop="dateModified">2016-03-21</time>
</a>
<a class="breadcrumb breadcrumb-link" href="/blog/categories/project/">project</a><a class="breadcrumb breadcrumb-link" href="/blog/categories/project/old-tech-new-ideas/">old tech new ideas</a>
<span class="hide" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
<span itemprop="name">
<a class="breadcrumb" href="https://plus.google.com/102455247906663845886" itemprop="url" rel="author">victor felder</a>
</span>
</span>
<div itemprop="publisher" itemscope itemtype="https://schema.org/Organization">
<meta itemprop="name" content="concise notes">
</div>
</div>
</div>
</nav>
<div class="article-inner">
<header class="article-header">
<h1 class="article-title" itemprop="headline">
How Many Websites Provide RSS / Web Syndication Feeds
</h1>
</header>
<div class="article-entry" itemprop="articleBody">
<p>I became interested in web syndication feeds recently. It’s an old technology nobody talks about anymore, but everybody still provides them. Taking a look at Google Trends shows us how much people have lost interest in RSS since its peak popularity in 2006:</p>
<script type="text/javascript" src="//www.google.com/trends/embed.js?hl=en-US&q=RSS&cmpt=q&tz=Etc/GMT-1&tz=Etc/GMT-1&content=1&cid=TIMESERIES_GRAPH_0&export=5&w=970&h=330"></script>
<p>It’s an old technology people seem to have lost interest in, but can we still rely on it? Are people abandoning web syndication? With the advent of CMS (eg. the huge popularity of WordPress), static website generators and publishing platforms (eg. Medium) which all provide syndication feeds <em>by default</em>, RSS doesn’t look dead to me.</p>
<p>This is how I became interested in finding how many websites actually do provide at least one XML syndication feed. <a href="http://trends.builtwith.com/feeds/RSS" target="_blank" rel="external">Builtwith</a> has a pretty nifty “trends” section and it states that 33% of the 1 million most visited websites have an RSS feed. While this gives us a pretty good idea, let’s see what we could do by ourselves.</p>
<p>We basically have two options here:</p>
<ol>
<li>Actually crawling the web - a costly and lengthy process</li>
<li>Relying on existing web crawl data</li>
</ol>
<p>Enters CommonCrawl. CommonCrawl is a non-profit founded on the exciting project of crawling tons of web pages and releasing the obtained dataset publicly and for free. Their latest dump was <a href="http://commoncrawl.org/2015/12/november-2015-crawl-archive-now-available/" target="_blank" rel="external">published in November 2015</a> and contains 1.82 billion web pages, amounting to over 151TB of highly compressed HTML. <a href="/blog/2016/02/02/fixing-hiring-through-rss/#f1">I half-jokingly said this before</a>: why not mining CommonCrawl to answer my own question?</p>
<p>A few weeks ago two colleagues of mine mentioned over lunch their desire to mine Common Crawl for their research, and it was a coincidence that I had been thinking about doing the same (though mostly for fun) for a few months. Wouldn’t have we been able to combine our efforts into a single “Common Crawl run”, I most probably wouldn’t have mined this dataset all by myself only to satisfy my curiosity.</p>
<h2 id="Working-with-CommonCrawl"><a href="#Working-with-CommonCrawl" class="headerlink" title="Working with CommonCrawl"></a>Working with CommonCrawl</h2><p>As mentioned before, the latest Common Crawl is 151TB of data hosted on S3. We decided to process it directly from a few EC2 spot instances using the <a href="http://webdatacommons.org/framework/" target="_blank" rel="external">WDC Extraction Framework</a>. Actually this choice was pretty obvious to us because the syndication feeds are not the only thing we wanted to extract from CommonCrawl. My colleagues were interested in extracting all <a href="https://developers.google.com/structured-data/schema-org" target="_blank" rel="external">structured data</a> embedded in any HTML page which is exactly what the WDC framework was designed for. We also wanted to extract all HTML anchors pointing to a Wikipedia page. We <a href="https://github.com/XI-lab/WDCFramework" target="_blank" rel="external">forked the WDC framework</a> and modified it to extract these different things all at once, thus only needing to go through the CommonCrawl dataset once.</p>
<p>Now, CommonCrawl dump consists of HTML web pages and we want to extract parts of it, eg. any <code>&lt;link rel=&quot;alternate&quot; href=&quot;/blog/atom.xml&quot; title=&quot;something&quot; type=&quot;application/atom+xml&quot;&gt;</code> with either RSS/Atom <code>type</code> attribute or <code>rel=&quot;alternate&quot;</code> attribute. Although <a href="http://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454" target="_blank" rel="external">parsing HTML with regex is a bad idea</a>, constructing a full-blown DOM tree out of each of these 1.82B pages would require far too much time and processing power.</p>
<p>Here’s the regular expression I used to match feeds:</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&lt;link[^&gt;]*(?:<span class="tag">\<span class="name">s</span></span>(?:type=[<span class="tag">\<span class="name">"</span></span>']?(application<span class="tag">\<span class="name">/</span></span>rss<span class="tag">\<span class="name">+</span></span>xml|application<span class="tag">\<span class="name">/</span></span>atom<span class="tag">\<span class="name">+</span></span>xml|application<span class="tag">\<span class="name">/</span></span>rss|application<span class="tag">\<span class="name">/</span></span>atom|application<span class="tag">\<span class="name">/</span></span>rdf<span class="tag">\<span class="name">+</span></span>xml|application<span class="tag">\<span class="name">/</span></span>rdf|text<span class="tag">\<span class="name">/</span></span>rss<span class="tag">\<span class="name">+</span></span>xml|text<span class="tag">\<span class="name">/</span></span>atom<span class="tag">\<span class="name">+</span></span>xml|text<span class="tag">\<span class="name">/</span></span>rss|text<span class="tag">\<span class="name">/</span></span>atom|text<span class="tag">\<span class="name">/</span></span>rdf<span class="tag">\<span class="name">+</span></span>xml|text<span class="tag">\<span class="name">/</span></span>rdf|text<span class="tag">\<span class="name">/</span></span>xml|application<span class="tag">\<span class="name">/</span></span>xml)[<span class="tag">\<span class="name">"</span></span>']?|rel=[<span class="tag">\<span class="name">"</span></span>']?(?:alternate)[<span class="tag">\<span class="name">"</span></span>']?))[^&gt;]*&gt;)"</span><br></pre></td></tr></table></figure>
<p>The WDC framework wrote its results to our S3 bucket in <code>csv.gz</code> format. For the whole run we budgeted 500USD for AWS EC2 instances. We spawned 100 <a href="https://aws.amazon.com/ec2/instance-types/#c3" target="_blank" rel="external">c3.4xlarge</a> EC2 instances (16 cores each) to run our modified WDC framework. Using spot instances saved us some money. It took around 30 hours to process the whole Common Crawl dump and costed less than 450USD, we were right on target.</p>
<p>Most interesting is of course post-processing the data, not extracting it. Here is what our results S3 bucket looks like together with a short explanation of what the result of our various extractions are:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2.3G    /WDC_112015/anchors</span><br><span class="line">60G     /WDC_112015/feeds</span><br><span class="line">54G     /WDC_112015/urls</span><br><span class="line">346G    /WDC_112015/data</span><br><span class="line">408G    /WDC_112015/anchor_pages</span><br><span class="line">37G     /WDC_112015/stats</span><br><span class="line">905G    total</span><br></pre></td></tr></table></figure>
<ul>
<li>(gzipped TSV) <code>anchors: page_url | anchor text | wikipedia_url</code></li>
<li>(gzipped TSV) <code>feeds: page_url | link_tag | link_type</code></li>
<li>(gzipped TSV) <code>urls: page_url</code></li>
<li>(gzipped text) <code>data: &lt;quintuplet&gt;\n</code> (subject, predicate, object, page, extractor_used)</li>
<li>(gzipped json) <code>anchor_pages: {&quot;url&quot;: page_url, &quot;content&quot;: full_html_page}\n</code></li>
<li>(gzipped TSV) <code>stats: arcFileName | arcFilePos | detectedMimeType | hostIp | html-head-meta | html-mf-adr | html-mf-geo | html-mf-hcalendar | html-mf-hcard | html-mf-hlisting | html-mf-hrecipe | html-mf-hresume | html-mf-hreview | html-mf-species | html-mf-xfn | html-microdata | html-rdfa | html-rdfa11 | mimeType | recordLength | referencedData | timestamp | totalTriples | uri</code></li>
</ul>
<p>First step was of course to sync this bucket locally and backup it to our NAS. Once this done and after having triggered security warnings at the network admins office for downloading almost 1TB at full speed, we proceeded to copy this data to <a href="http://daplab.ch" target="_blank" rel="external">DAPLAB</a> hadoop cluster. DAPLAB is an awesome project aiming at providing a powerful data processing cluster on a freemium and premium basis for companies that cannot afford their own cluster, for researchers and scientists, etc. They also organize weekly hacking sessions to which anyone can attend and get access to the cluster for free. Our research lab has been partnering with DAPLAB since the beginning and DAPLAB infrastructure is a very nice complement to our lab’s hadoop cluster. (No more advertising in this post I promise.)</p>
<p>We now have all the data on HDFS, let’s process it. Keep in mind this blog post is about the XML syndication feeds and not how and what we did with the other things we extracted (anchors, RDF triples, etc).</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[vfelder@daplab ~]$ hdfs dfs -ls /data/WDC_112015/data/feeds | head -n2</span><br><span class="line">Found 35669 items</span><br><span class="line">1.7M 2016-02-28 15:21 /data/WDC_112015/data/feeds/ex_common-crawl_crawl-data_CC-MAIN-2015-48_segments_1448398444047.40_warc_CC-MAIN-20151124205404-00000-ip-10-71-132-137.ec2.internal.warc.gz.csv.gz</span><br></pre></td></tr></table></figure>
<p>We have 35,669 gzipped TSV files. We don’t want to work with this format and compression because gzip is slow and CSV/TSV is not ideal to query the data. Also, 35k files is a bit too much for a ~10 nodes cluster. We will convert these 35k gzipped TSV to 1,000 snappy-compressed parquet files.</p>
<p>Why converting 35k gunzip csv files to 1,000 snappy parquet files?</p>
<ul>
<li><a href="https://parquet.apache.org/" target="_blank" rel="external">Parquet</a> is better suited for querying (eg. using Hive) than CSV:<ul>
<li>Parquet files contain their schema, CSV don’t.</li>
<li>Parquet files store data by column, CSV is row-based.</li>
<li>Columns of a parquet file are compressed (each column being compressed according to its data type).</li>
</ul>
</li>
<li>Snappy also offers great advantages, here compared to gzip:<ul>
<li>Snappy compression is orders of magnitude faster than gzip.</li>
<li>Snappy happily trades compression against read/write speed. After all, when running a job that loads terabytes of data from HDFS on a cluster, we care more about fast read/write than about sparing a few TB.</li>
</ul>
</li>
<li>Less files means more throughput (as long as we have more files than processing cores of course). If we assume decompressing a file takes 100ms, the decompression alone for all 35k files will take a cumulated one hour. Of course the whole process will be distributed and run in parallel but it still constitutes overhead compared to working on only 1,000 files.</li>
<li>We could have chosen Avro, but our schemas being very basic and our need being to query columns, Parquet made more sense.</li>
</ul>
<p>Remember our TSV “schema” is <code>page_url | link_tag | link_type</code>. It’s a good thing we saved the whole <code>&lt;link</code> tag because I realized after the EC2 run it might be interesting to extract a few other possible attributes. I wrote a short Scala script for Spark to extract these things <strong>and</strong> go from <code>csv.gz</code> to 1,000 <code>.snappy.parquet</code>.</p>
<figure class="highlight scala"><figcaption><span>feedsTransform.scala</span><a href="https://github.com/vhf/wdctools/blob/master/src/main/scala/info/exascale/wdctools/feedsTransform.scala" target="_blank" rel="external">link</a></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> info.exascale.wdctools</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.netaporter.uri.<span class="type">Uri</span></span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.types.&#123;<span class="type">StringType</span>, <span class="type">StructField</span>, <span class="type">StructType</span>&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>, sql&#125;</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions._</span><br><span class="line"><span class="keyword">import</span> scala.language.postfixOps</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">feedsTransform</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"TransformFeeds"</span>)</span><br><span class="line">      .set(<span class="string">"spark.sql.parquet.compression.codec"</span>, <span class="string">"snappy"</span>) <span class="comment">// snappy compression for parquet</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line">    <span class="keyword">val</span> sqlContext = <span class="keyword">new</span> sql.<span class="type">SQLContext</span>(sc)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// schema for the CSV we'll load</span></span><br><span class="line">    <span class="keyword">val</span> feedSchema = <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"page"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"tag"</span>, <span class="type">StringType</span>, <span class="literal">true</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">"type"</span>, <span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// read the CSV with our schema using databricks' spark-csv</span></span><br><span class="line">    <span class="keyword">val</span> df = sqlContext</span><br><span class="line">      .read</span><br><span class="line">      .format(<span class="string">"com.databricks.spark.csv"</span>)</span><br><span class="line">      .option(<span class="string">"header"</span>, <span class="string">"false"</span>)</span><br><span class="line">      .schema(feedSchema)</span><br><span class="line">      .load(<span class="string">"/data/WDC_112015/data/feeds/*.csv.gz"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// extract from each page url their hostname</span></span><br><span class="line">    <span class="comment">// some URLs (eg. http://example.com:/foo.html) made Uri crash, I had to implement a regex</span></span><br><span class="line">    <span class="comment">// based check</span></span><br><span class="line">    <span class="keyword">val</span> hostnamePattern = <span class="string">"((\\/\\/|https\\:\\/\\/|http\\:\\/\\/)([^\\/\\:]+))"</span>r</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> getHost: (<span class="type">String</span> =&gt; <span class="type">String</span>) = (page: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> preFiltered = hostnamePattern findFirstIn page</span><br><span class="line">      <span class="keyword">if</span> (preFiltered.isEmpty) &#123;</span><br><span class="line">        println(<span class="string">s"prefiltering failed: <span class="subst">$page</span>"</span>)</span><br><span class="line">        <span class="string">""</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">val</span> preFilteredString = preFiltered.get</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="keyword">val</span> host = <span class="type">Uri</span>.parse(preFilteredString).host</span><br><span class="line">          <span class="keyword">if</span> (host.isEmpty) &#123;</span><br><span class="line">            println(<span class="string">s"parsing failed: <span class="subst">$page</span> prefiltered as: <span class="subst">$preFilteredString</span>"</span>)</span><br><span class="line">            <span class="string">""</span></span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            host.get</span><br><span class="line">          &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">Throwable</span> =&gt; &#123;</span><br><span class="line">            <span class="keyword">val</span> exception = e.toString</span><br><span class="line">            println(<span class="string">s"caught <span class="subst">$exception</span>"</span>)</span><br><span class="line">            <span class="string">""</span></span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// extract rel="…"</span></span><br><span class="line">    <span class="keyword">val</span> relPattern = <span class="string">".*rel=[\"']?([^'\"]*)[\"']?.*"</span>.r</span><br><span class="line">    <span class="keyword">val</span> getRel: (<span class="type">String</span> =&gt; <span class="type">String</span>) = (tag: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      tag <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// pattern-matching regex matches is so nice</span></span><br><span class="line">        <span class="keyword">case</span> relPattern(captured) =&gt; captured</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">""</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// extract href="…"</span></span><br><span class="line">    <span class="keyword">val</span> hrefPattern = <span class="string">".*href=[\"']?([^'\"]*)[\"']?.*"</span>.r</span><br><span class="line">    <span class="keyword">val</span> getHref: (<span class="type">String</span> =&gt; <span class="type">String</span>) = (tag: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      tag <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> hrefPattern(captured) =&gt; captured</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">""</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// extract title="…"</span></span><br><span class="line">    <span class="keyword">val</span> titlePattern = <span class="string">".*title=[\"']?([^'\"]*)[\"']?.*"</span>.r</span><br><span class="line">    <span class="keyword">val</span> getTitle: (<span class="type">String</span> =&gt; <span class="type">String</span>) = (tag: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      tag <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> titlePattern(captured) =&gt; captured</span><br><span class="line">        <span class="keyword">case</span> _ =&gt; <span class="string">""</span></span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// transform to lowercase</span></span><br><span class="line">    <span class="keyword">val</span> getLCType: (<span class="type">String</span> =&gt; <span class="type">String</span>) = (str: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">      str.toLowerCase</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// create spark sql user-defined functions for each of these scala function</span></span><br><span class="line">    <span class="keyword">val</span> sqlGetHost = udf(getHost)</span><br><span class="line">    <span class="keyword">val</span> sqlGetRel = udf(getRel)</span><br><span class="line">    <span class="keyword">val</span> sqlGetHref = udf(getHref)</span><br><span class="line">    <span class="keyword">val</span> sqlGetTitle = udf(getTitle)</span><br><span class="line">    <span class="keyword">val</span> sqlGetLCType = udf(getLCType)</span><br><span class="line"></span><br><span class="line">    df</span><br><span class="line">      <span class="comment">// add hostname column based on page column</span></span><br><span class="line">      .withColumn(<span class="string">"hostname"</span>, sqlGetHost(col(<span class="string">"page"</span>)))</span><br><span class="line">      <span class="comment">// add rel column based on tag column</span></span><br><span class="line">      .withColumn(<span class="string">"rel"</span>, sqlGetRel(col(<span class="string">"tag"</span>)))</span><br><span class="line">      <span class="comment">// add href column based on tag column</span></span><br><span class="line">      .withColumn(<span class="string">"href"</span>, sqlGetHref(col(<span class="string">"tag"</span>)))</span><br><span class="line">      <span class="comment">// add title column based on tag column</span></span><br><span class="line">      .withColumn(<span class="string">"title"</span>, sqlGetTitle(col(<span class="string">"tag"</span>)))</span><br><span class="line">      <span class="comment">// replace type column with its lowercase version</span></span><br><span class="line">      .withColumn(<span class="string">"type"</span>, sqlGetLCType(col(<span class="string">"type"</span>)))</span><br><span class="line">      <span class="comment">// only output 1,000 parquet files from the 35k csv.gz input files</span></span><br><span class="line">      .coalesce(<span class="number">1000</span>)</span><br><span class="line">      <span class="comment">// write as parquet to my hdfs home folder</span></span><br><span class="line">      .write.parquet(<span class="string">"/user/vfelder/feeds/feedsparsed.parquet/"</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>These 60G total / 35k <code>.csv.gz</code>, once converted to 1k <code>.snappy.parquet</code> with 4 additional columns, now take 187G. Not bad.</p>
<p>We could now create a Hive table like this:</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> xi_wdc.feeds (page <span class="keyword">STRING</span>, <span class="keyword">type</span> <span class="keyword">STRING</span>, tag <span class="keyword">STRING</span>, hostname <span class="keyword">STRING</span>, rel <span class="keyword">STRING</span>, href <span class="keyword">STRING</span>, title <span class="keyword">STRING</span>) <span class="keyword">STORED</span> <span class="keyword">AS</span> PARQUET LOCATION <span class="string">'/user/vfelder/feeds/feedsparsed.parquet/'</span>;</span><br></pre></td></tr></table></figure>
<p><code>EXTERNAL</code> means the data for this table isn’t moved by Hive to the tables location, it just stays where it is and Hive loads the data directly from these files. Should we drop this table the data won’t be affected.</p>
<p>I also performed the above <code>csv.gz</code> to <code>snappy.parquet</code> conversion + hostname extraction for <code>/data/WDC_112015/data/urls/</code> which contains all crawled URLs. The Scala program to do this is very similar to the one listed here and can be found <a href="https://github.com/vhf/wdctools" target="_blank" rel="external">here</a>.</p>
<p>Hive is nice to run some queries, but Spark SQL is equally nice and generally offers better performances. It also allows me to run queries directly from the ipython notebook I’m writing this blog post from, get the queries results and rework them directly in python.</p>
<p>The first part of this blog post was writting in Markdown in my ipython notebook running pyspark, the rest of this post is playing with the data directly from ipython, writing pyspark.sql queries and executing them directly from ipython, running them with pyspark over the cluster.</p>
<p>I got this handy *sh alias I’m using to spawn a notebook in a screen:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ipyspark=<span class="string">'IPYTHON_OPTS="notebook --no-browser --ip=localhost --port=1339" pyspark --master yarn-master --conf spark.ui.port=$(shuf -i 2000-65000 -n 1) --num-executors 20 --executor-cores 2 --driver-memory 16g --executor-memory 16g'</span></span><br></pre></td></tr></table></figure>
<h1 id="Let’s-work-with-our-extracted-feeds-and-all-crawled-URLs"><a href="#Let’s-work-with-our-extracted-feeds-and-all-crawled-URLs" class="headerlink" title="Let’s work with our extracted feeds and all crawled URLs"></a>Let’s work with our extracted feeds and all crawled URLs</h1><p>First, import some spark-sql libs.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SQLContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">h</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="string">"""helper function to display numbers in a human-readable way"""</span></span><br><span class="line">    <span class="keyword">return</span> <span class="string">'&#123;:,&#125;'</span>.format(n)</span><br></pre></td></tr></table></figure>
<p><code>sc</code> is spark context, it’s already there because we’re running ipython on pyspark.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqlContext = SQLContext(sc)</span><br></pre></td></tr></table></figure>
<p>We stored the feeds and urls as snappy compressed parquet files. All we have to do is read them as parquet, everything else is taken care of.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">feeds = sqlContext.read.parquet(<span class="string">'/user/vfelder/feeds/feedsparsed.parquet/'</span>)</span><br><span class="line">urls = sqlContext.read.parquet(<span class="string">'/user/vfelder/urls/urlsparsed.parquet/'</span>)</span><br></pre></td></tr></table></figure>
<p>Common Crawl said their dump had 1.82B URLs, let’s check if that’s also the number of web pages we used for our extraction.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;<span class="string">'total number of urls'</span>: h(urls.count())&#125;</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;total number of urls&apos;: &apos;1,823,130,936&apos;}
</code></pre><p>Nice, it seems the data matches.</p>
<p>Feeds and URLs files include these two columns : <code>page</code> and <code>hostname</code>, respectively the original URL of the web page and the hostname of this URL. A single web page can provide several feeds but we’re only interested by the number of websites which provide at least one feed, so we take the distinct hostnames. Same with URLs: we crawled a lot of URLs but we are only interested by the number of distinct hostnames, to compare the two numbers.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">distinct_feeds_count = feeds.select(<span class="string">'hostname'</span>).distinct().count()</span><br><span class="line">distinct_urls_count = urls.select(<span class="string">'hostname'</span>).distinct().count()</span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'hostnames with at least one feed'</span>: h(distinct_feeds_count),</span><br><span class="line">    <span class="string">'hostnames processed'</span>: h(distinct_urls_count)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;hostnames processed&apos;: &apos;25,243,438&apos;,
 &apos;hostnames with at least one feed&apos;: &apos;10,294,833&apos;}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">'&#123;:0.2f&#125;%'</span>.format(float(distinct_feeds_count)/distinct_urls_count*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&apos;40.78%&apos;
</code></pre><p>Ok, around 40% of the crawled websites provide at least one XML feed.</p>
<p>But on average, how many feeds per webpage?</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">total_feeds_count = feeds.count()</span><br><span class="line">total_urls_count = urls.count()</span><br><span class="line">float(total_feeds_count)/total_urls_count</span><br></pre></td></tr></table></figure>
<pre><code>1.942656384170972
</code></pre><p>Now it would be interesting to see which standards these feeds implement.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Column</span><br><span class="line">types_grouped = feeds</span><br><span class="line">    .select(<span class="string">'type'</span>)</span><br><span class="line">    .where(col(<span class="string">'type'</span>) != <span class="string">' '</span>)</span><br><span class="line">    .groupBy(<span class="string">'type'</span>)</span><br><span class="line">    .count()</span><br><span class="line">    .orderBy(desc(<span class="string">'count'</span>))</span><br><span class="line">types_grouped.show()</span><br></pre></td></tr></table></figure>
<pre><code>+--------------------+---------+
|                type|    count|
+--------------------+---------+
| application/rss+xml|877891180|
|application/atom+xml|215504961|
|     application/xml| 20276729|
|            text/xml| 17086738|
| application/rdf+xml| 14669025|
|    application/atom|   545717|
|     application/rdf|   425259|
|     application/rss|   108139|
|            text/rdf|     9637|
|        text/rss+xml|      245|
|            text/rss|      125|
|        text/rdf+xml|       14|
+--------------------+---------+
</code></pre><p>Let’s do some basic stats:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">rss_feeds = types_grouped</span><br><span class="line">    .filter(<span class="string">'type LIKE "%rss%"'</span>)</span><br><span class="line">    .agg(&#123;<span class="string">'count'</span>: <span class="string">'sum'</span>&#125;)</span><br><span class="line">    .collect()</span><br><span class="line">rss_total = rss_feeds[<span class="number">0</span>].asDict().values()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">atom_feeds = types_grouped</span><br><span class="line">    .filter(<span class="string">'type LIKE "%atom%"'</span>)</span><br><span class="line">    .agg(&#123;<span class="string">'count'</span>: <span class="string">'sum'</span>&#125;)</span><br><span class="line">    .collect()</span><br><span class="line">atom_total = atom_feeds[<span class="number">0</span>].asDict().values()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">feeds_with_type = types_grouped</span><br><span class="line">    .agg(&#123;<span class="string">'count'</span>: <span class="string">'sum'</span>&#125;)</span><br><span class="line">    .collect()</span><br><span class="line">feeds_with_type_total = feeds_with_type[<span class="number">0</span>].asDict().values()[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'RSS'</span>: h(rss_total),</span><br><span class="line">    <span class="string">'Atom'</span>: h(atom_total)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;Atom&apos;: &apos;216,050,678&apos;, &apos;RSS&apos;: &apos;877,999,689&apos;}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rss_pc = float(rss_total)/feeds_with_type_total*<span class="number">100</span></span><br><span class="line">atom_pc = float(atom_total)/feeds_with_type_total*<span class="number">100</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">'% RSS'</span>: <span class="string">'&#123;:0.2f&#125;%'</span>.format(rss_pc),</span><br><span class="line">    <span class="string">'% Atom'</span>: <span class="string">'&#123;:0.2f&#125;%'</span>.format(atom_pc),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<pre><code>{&apos;% Atom&apos;: &apos;18.84%&apos;, &apos;% RSS&apos;: &apos;76.58%&apos;}
</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alternate_total = feeds</span><br><span class="line">    .select(<span class="string">'rel'</span>, <span class="string">'type'</span>)</span><br><span class="line">    .where(<span class="string">'rel LIKE "%alternate%" AND (type LIKE "%rss%" OR type LIKE "%atom%")'</span>)</span><br><span class="line">    .count()</span><br><span class="line"><span class="string">'&#123;:0.2f&#125;%'</span>.format(float(alternate_total)/(rss_total+atom_total)*<span class="number">100</span>)</span><br></pre></td></tr></table></figure>
<pre><code>&apos;97.09%&apos;
</code></pre><p>That’s about it, my questions have been answered!</p>
<h4 id="Let’s-quickly-recap"><a href="#Let’s-quickly-recap" class="headerlink" title="Let’s quickly recap:"></a>Let’s quickly recap:</h4><ul>
<li>Common Crawl is an excellent free dataset.</li>
<li>WDC Framework is a cool tool but requires some tweaking.</li>
<li>It is possible to extract a fair amount of data from this 151TB dump for less than 500USD of EC2.</li>
<li>Out of 1.82B URLs we got 25.2M different hosts.</li>
<li>Processing this data is really fast on DAPLAB’s cluster with my ipython+pyspark setup. This last (<code>alternate_total</code>) query takes &lt;10s.</li>
<li><strong>40.8% of these hostnames provide a web syndication feed.</strong></li>
<li>RSS is the most popular syndication format by far with 76.6%, Atom is at 18.8%.</li>
<li>97.09% of the <code>&lt;link</code> tags pointing to an RSS or Atom feed also specify <code>rel=&quot;alternate&quot;</code>, as the HTML standard recommends.</li>
</ul>
</div>
<footer class="article-footer">
<div class="row">
<div class="col s6">
<ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/ipython/">ipython</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/rss/">rss</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/scala/">scala</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/blog/tags/spark/">spark</a></li></ul>
&nbsp;
</div>
<div class="col s6 right-align">
<div class="article-footer-right">
<a data-url="https://vhf.github.io/blog/2016/03/21/rss-usage-on-the-web/" data-id="cioa6p5oe001qzzabdhul4idl" class="article-share-link">
<span class="text">share</span>
</a>
or
<a href="https://github.com/vhf/concise-notes/edit/master/source/_posts/2016-03-21-rss-usage-on-the-web.md" class="article-suggest-modification">
<span class="text">suggest a modification</span>
</a>
</div>
</div>
</div>
</footer>
</div>
<div class="row post-navigation">
<div class="col s12 m6 push-m6">
<div class="post-nav">
<div class="post-nav-title blue-grey darken-1 right center-align">
<span class="white-text">Newer</span>
</div>
<div class="post-nav-content post-nav-prev clear right-align">
<a href="/blog/2016/05/16/ngw-2016/">
!!Con and a week back at the Recurse Center
</a>
</div>
</div>
</div>
<div class="col s12 m6 pull-m6">
<div class="post-nav">
<div class="post-nav-title blue-grey darken-1 left center-align">
<span class="white-text">Older</span>
</div>
<div class="post-nav-content post-nav-next clear">
<a href="/blog/2016/02/06/navigating-the-ecmascript-2015-language-specification/">
Navigating the ECMAScript® 2015 Language Specification
</a>
</div>
</div>
</div>
</div>
</article>
</main>
</div>
<footer class="page-footer blue-grey darken-3">
<div class="row">
<div class="col l3 m6 s12">
<h5 class="white-text">Categories</h5>
<ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/programming/">programming</a><span class="category-list-count">12</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/programming/JavaScript/">JavaScript</a><span class="category-list-count">9</span></li></ul></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/project/">project</a><span class="category-list-count">3</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/blog/categories/project/old-tech-new-ideas/">old tech new ideas</a><span class="category-list-count">2</span></li><li class="category-list-item"><a class="category-list-link" href="/blog/categories/project/tools/">tools</a><span class="category-list-count">1</span></li></ul></li></ul>
</div>
<div class="col l3 m6 s12">
<h5 class="white-text">Tags</h5>
<ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/babel/">babel</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/chrome/">chrome</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/crankshaft/">crankshaft</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/es2015/">es2015</a><span class="tag-list-count">7</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/file-systems/">file systems</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/git/">git</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/github/">github</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/hiring/">hiring</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/ipython/">ipython</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/javascript/">javascript</a><span class="tag-list-count">9</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/nodejs/">nodejs</a><span class="tag-list-count">6</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/osx/">osx</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/programming-language-design/">programming language design</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/promises/">promises</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/rss/">rss</a><span class="tag-list-count">2</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/scala/">scala</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/spark/">spark</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/blog/tags/v8/">v8</a><span class="tag-list-count">6</span></li></ul>
</div>
<div class="col l3 m6 s12">
<h5 class="white-text">Archives</h5>
<ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/11/">November 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/05/">May 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/03/">March 2016</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/02/">February 2016</a><span class="archive-list-count">3</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2016/01/">January 2016</a><span class="archive-list-count">6</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/12/">December 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/11/">November 2015</a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/blog/archives/2015/10/">October 2015</a><span class="archive-list-count">2</span></li></ul>
</div>
<div class="col l3 m6 s12">
<h5 class="white-text">Recent posts</h5>
<ul>
<li>
<a href="/blog/2016/11/28/javascript-engines-hidden-classes/">JavaScript Engines Hidden Classes (and Why You Should Keep Them in Mind)</a>
</li>
<li>
<a href="/blog/2016/05/16/ngw-2016/">!!Con and a week back at the Recurse Center</a>
</li>
<li>
<a href="/blog/2016/03/21/rss-usage-on-the-web/">How Many Websites Provide RSS / Web Syndication Feeds</a>
</li>
<li>
<a href="/blog/2016/02/06/navigating-the-ecmascript-2015-language-specification/">Navigating the ECMAScript® 2015 Language Specification</a>
</li>
<li>
<a href="/blog/2016/02/04/the-dangers-of-hfs-for-git-repositories/">The dangers of HFS+ for git repositories</a>
</li>
</ul>
</div>
</div>
<div class="footer-copyright blue-grey darken-3">
unless otherwise stated, all content &amp; design &copy; copyright 2016 victor felder &mdash; <span class="post-count">25.9k words total</span><br>
</div>
</footer>
</div>
<nav class="side-nav">
<ul id="nav-mobile">
<li><a class="main-nav-link" href="/blog/">Home</a></li>
<li><a class="main-nav-link" href="/blog/archives">Archives</a></li>
<li><a class="main-nav-link" href="/blog/contact">Contact</a></li>
<li><a class="main-nav-link" href="https://vhf.github.io">About</a></li>
<li><a id="mobile-menu-close" class="main-nav-link" href="#">Close the menu</a></li>
</ul>
</nav>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery/2.1.4/jquery.min.js"></script>
<script src="/blog/js/script.js"></script>
</body>
</html>